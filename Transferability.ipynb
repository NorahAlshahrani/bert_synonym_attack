{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d5f11-d605-4111-b7d6-14bb722b82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, torch\n",
    "from transformers import logging as hfl\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import logging, warnings, tensorflow as tf\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "\n",
    "\n",
    "disable_progress_bars()\n",
    "start_time = time.time()\n",
    "hfl.set_verbosity_error()\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdcb40b-ca5e-4984-b9f9-9d46429bb4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model, dataset):\n",
    "    if model == 'bert' and dataset == 'hard':\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        tokenizer = BertTokenizer.from_pretrained('models/bert-base-arabertv2')\n",
    "        model = BertForSequenceClassification.from_pretrained('models/BERThard', num_labels=4)\n",
    "        return model, tokenizer\n",
    "\n",
    "    elif model == 'cnn' and dataset == 'hard':\n",
    "        import pickle\n",
    "        from huggingface_hub import from_pretrained_keras\n",
    "        tokenizer = pickle.load(open('tokenizers/tokenizerCNNhard.pickle', 'rb'))\n",
    "        model = from_pretrained_keras('models/2dCNNhard')\n",
    "        return model, tokenizer\n",
    "\n",
    "    elif model == 'bilstm' and dataset == 'hard':\n",
    "        import pickle\n",
    "        from huggingface_hub import from_pretrained_keras\n",
    "        tokenizer = pickle.load(open('tokenizers/tokenizerbiLSTMhard.pickle', 'rb'))\n",
    "        model = from_pretrained_keras('models/biLSTMhard')\n",
    "        return model, tokenizer\n",
    "\n",
    "    elif model == 'bert' and dataset == 'msda':\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        tokenizer = BertTokenizer.from_pretrained('models/bert-base-arabertv2')\n",
    "        model = BertForSequenceClassification.from_pretrained('models/BERTmsda', num_labels=3)\n",
    "        return model, tokenizer\n",
    "\n",
    "    elif model == 'cnn' and dataset == 'msda':\n",
    "        import pickle\n",
    "        from huggingface_hub import from_pretrained_keras\n",
    "        tokenizer = pickle.load(open('tokenizers/tokenizerCNNmsda.pickle', 'rb'))\n",
    "        model = from_pretrained_keras('models/2dCNNmsda')\n",
    "        return model, tokenizer\n",
    "\n",
    "    elif model == 'bilstm' and dataset == 'msda':\n",
    "        import pickle\n",
    "        from huggingface_hub import from_pretrained_keras\n",
    "        tokenizer = pickle.load(open('tokenizers/tokenizerbiLSTMmsda.pickle', 'rb'))\n",
    "        model = from_pretrained_keras('models/biLSTMmsda')\n",
    "        return model, tokenizer\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: load() function takes 2 arguments: \\n  \\\n",
    "        model={bert, cnn, or bilstm}, \\n\\t  dataset={hard or msda}\")\n",
    "\n",
    "\n",
    "\n",
    "def predict(text, model):\n",
    "    import numpy as np\n",
    "    if model == 'bert_hard':\n",
    "        model, tokenizer = load('bert', 'hard')\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "        outputs = model(**inputs).logits\n",
    "        id2label = {0: 'Poor', 1: 'Fair', 2: 'Good', 3: 'Excellent'}\n",
    "        predicted_class_id = outputs.argmax().item()\n",
    "        preds = outputs.softmax(dim=-1).tolist()\n",
    "        predicted_score = np.max(preds)\n",
    "        return id2label[predicted_class_id], predicted_score\n",
    "\n",
    "    elif model == 'cnn_hard':\n",
    "        import torch, numpy as np, tensorflow as tf\n",
    "        from keras_preprocessing.sequence import pad_sequences\n",
    "        model, tokenizer = load('cnn', 'hard')\n",
    "        inputs = tokenizer.texts_to_sequences([text])\n",
    "        inputs = pad_sequences(inputs, maxlen=512)\n",
    "        outputs = torch.from_numpy(model.predict(inputs, verbose=0))\n",
    "        id2label = {0: 'Poor', 1: 'Fair', 2: 'Good', 3: 'Excellent'}\n",
    "        predicted_class_id = outputs.argmax().item()\n",
    "        preds = outputs.softmax(dim=-1).tolist()\n",
    "        predicted_score = np.max(preds)\n",
    "        return id2label[predicted_class_id], predicted_score\n",
    "\n",
    "    elif model == 'bilstm_hard':\n",
    "        import torch, numpy as np, tensorflow as tf\n",
    "        from keras_preprocessing.sequence import pad_sequences\n",
    "        model, tokenizer = load('bilstm', 'hard')\n",
    "        inputs = tokenizer.texts_to_sequences([text])\n",
    "        inputs = pad_sequences(inputs, maxlen=512)\n",
    "        outputs = torch.from_numpy(model.predict(inputs, verbose=0))\n",
    "        id2label = {0: 'Poor', 1: 'Fair', 2: 'Good', 3: 'Excellent'}\n",
    "        predicted_class_id = outputs.argmax().item()\n",
    "        preds = outputs.softmax(dim=-1).tolist()\n",
    "        predicted_score = np.max(preds)\n",
    "        return id2label[predicted_class_id], predicted_score\n",
    "\n",
    "    elif model == 'bert_msda':\n",
    "        model, tokenizer = load('bert', 'msda')\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs).logits\n",
    "        id2label = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "        predicted_class_id = outputs.argmax().item()\n",
    "        preds = outputs.softmax(dim=-1).tolist()\n",
    "        predicted_score = np.max(preds)\n",
    "        return id2label[predicted_class_id], predicted_score\n",
    "\n",
    "    elif model == 'cnn_msda':\n",
    "        import torch, numpy as np, tensorflow as tf\n",
    "        from keras_preprocessing.sequence import pad_sequences\n",
    "        model, tokenizer = load('cnn', 'msda')\n",
    "        inputs = tokenizer.texts_to_sequences([text])\n",
    "        inputs = pad_sequences(inputs, maxlen=330)\n",
    "        outputs = torch.from_numpy(model.predict(inputs, verbose=0))\n",
    "        id2label = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "        predicted_class_id = outputs.argmax().item()\n",
    "        preds = outputs.softmax(dim=-1).tolist()\n",
    "        predicted_score = np.max(preds)\n",
    "        return id2label[predicted_class_id], predicted_score\n",
    "\n",
    "    elif model == 'bilstm_msda':\n",
    "        import torch, numpy as np, tensorflow as tf\n",
    "        from keras_preprocessing.sequence import pad_sequences\n",
    "        model, tokenizer = load('bilstm', 'msda')\n",
    "        inputs = tokenizer.texts_to_sequences([text])\n",
    "        inputs = pad_sequences(inputs, maxlen=330)\n",
    "        outputs = torch.from_numpy(model.predict(inputs, verbose=0))\n",
    "        id2label = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "        predicted_class_id = outputs.argmax().item()\n",
    "        preds = outputs.softmax(dim=-1).tolist()\n",
    "        predicted_score = np.max(preds)\n",
    "        return id2label[predicted_class_id], predicted_score\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: predict() function takes 3 arguments: \\n  \\\n",
    "        text={str}, \\n\\t  model={bert, cnn, or bilstm}, \\n\\t  dataset={hard or msda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f898af4-beee-4538-b93e-464da56166d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transferability(victim_model, advs_model_1, advs_model_2, n_samples):\n",
    "    import pandas as pd\n",
    "    model_1_dataframe = pd.read_csv(advs_model_1)\n",
    "    model_1_dataframe = model_1_dataframe.sample(n_samples)\n",
    "\n",
    "    model_1_dataframe['predication_score'] = model_1_dataframe['example'].apply(lambda example: predict(example, victim_model)[1])\n",
    "    model_1_dataframe['adv_predication_score'] = model_1_dataframe['adversarial_example'].apply(lambda example: predict(example, victim_model)[1])\n",
    "\n",
    "    model_1_org_scores_mean = model_1_dataframe['predication_score'].mean()*100\n",
    "    model_1_adv_scores_mean = model_1_dataframe['adv_predication_score'].mean()*100\n",
    "\n",
    "    model_2_dataframe = pd.read_csv(advs_model_2)\n",
    "    model_2_dataframe = model_2_dataframe.sample(n_samples)\n",
    "\n",
    "    model_2_dataframe['predication_score'] = model_2_dataframe['example'].apply(lambda example: predict(example, victim_model)[1])\n",
    "    model_2_dataframe['adv_predication_score'] = model_2_dataframe['adversarial_example'].apply(lambda example: predict(example, victim_model)[1])\n",
    "\n",
    "    model_2_org_scores_mean = model_2_dataframe['predication_score'].mean()*100\n",
    "    model_2_adv_scores_mean = model_2_dataframe['adv_predication_score'].mean()*100\n",
    "\n",
    "    delta_model_1 = model_1_org_scores_mean-model_1_adv_scores_mean\n",
    "    delta_model_2 = model_2_org_scores_mean-model_2_adv_scores_mean\n",
    "\n",
    "    return (model_1_org_scores_mean, model_1_adv_scores_mean, delta_model_1), (model_2_org_scores_mean, model_2_adv_scores_mean, delta_model_2)\n",
    "\n",
    "\n",
    "def test_transferability1(victim_model, advs_model_1, advs_model_2, n_samples):\n",
    "    import pandas as pd\n",
    "    model_1_dataframe = pd.read_csv(advs_model_1)\n",
    "    model_1_dataframe = model_1_dataframe.sample(n_samples)\n",
    "\n",
    "    model_1_dataframe['predication_label'] = model_1_dataframe['example'].apply(lambda example: predict(example, victim_model)[0])\n",
    "    model_1_dataframe['adv_predication_label'] = model_1_dataframe['adversarial_example'].apply(lambda example: predict(example, victim_model)[0])\n",
    "    model_1_dataframe['accuracy'] = model_1_dataframe.apply(lambda x: 1 if x['predication_label'] == x['adv_predication_label'] else 0, axis=1)\n",
    "    model_1_accuracy = model_1_dataframe['accuracy'].mean()*100\n",
    "\n",
    "    model_2_dataframe = pd.read_csv(advs_model_2)\n",
    "    model_2_dataframe = model_2_dataframe.sample(n_samples)\n",
    "    model_2_dataframe['predication_label'] = model_2_dataframe['example'].apply(lambda example: predict(example, victim_model)[0])\n",
    "    model_2_dataframe['adv_predication_label'] = model_2_dataframe['adversarial_example'].apply(lambda example: predict(example, victim_model)[0])\n",
    "    model_2_dataframe['accuracy'] = model_2_dataframe.apply(lambda x: 1 if x['predication_label'] == x['adv_predication_label'] else 0, axis=1)\n",
    "    model_2_accuracy = model_2_dataframe['accuracy'].mean()*100\n",
    "\n",
    "    return (model_1_accuracy, 100-model_1_accuracy), (model_2_accuracy, 100-model_2_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e1333b-5883-4602-a839-bf87adcf734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\t\\t\\t\\t adv_CNN_hard \\t\\t\\t\\t\\t\\t adv_biLSTM_hard\")\n",
    "print(\"BERT_hard \\t << \\t\", test_transferability(\"bert_hard\", \"transferability/cnn_hard.csv\", \"transferability/bilstm_hard.csv\", 245), \"\\n\")\n",
    "\n",
    "print(\"\\t\\t\\t\\t\\t\\t adv_CNN_msda \\t\\t\\t\\t\\t\\t adv_biLSTM_msda\")\n",
    "print(\"BERT_msda \\t << \\t\", test_transferability(\"bert_msda\", \"transferability/cnn_msda.csv\", \"transferability/bilstm_msda.csv\", 245), \"\\n\")\n",
    "\n",
    "print(\"\\t\\t\\tadv_CNN_hard \\t adv_biLSTM_hard\")\n",
    "print(\"BERT_hard \\t << \\t\", test_transferability1(\"bert_hard\", \"transferability/cnn_hard.csv\", \"transferability/bilstm_hard.csv\", 245), \"\\n\")\n",
    "\n",
    "print(\"\\t\\t\\tadv_CNN_msda \\t adv_biLSTM_msda\")\n",
    "print(\"BERT_msda \\t << \\t\", test_transferability1(\"bert_msda\", \"transferability/cnn_msda.csv\", \"transferability/bilstm_msda.csv\", 245), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67194219-1a92-42b8-9c77-6cf552546c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\t\\t\\t\\t adv_BERT_hard \\t\\t\\t\\t\\t\\t adv_biLSTM_hard\")\n",
    "print(\"CNN_hard \\t << \\t\", test_transferability(\"cnn_hard\", \"transferability/bert_hard.csv\", \"transferability/bilstm_hard.csv\", 245), \"\\n\")\n",
    "\n",
    "print(\"\\t\\t\\t\\t\\t\\t adv_BERT_msda \\t\\t\\t\\t\\t\\t adv_biLSTM_msda\")\n",
    "print(\"CNN_msda \\t << \\t\", test_transferability(\"cnn_msda\", \"transferability/bert_msda.csv\", \"transferability/bilstm_msda.csv\", 245), \"\\n\")\n",
    "\n",
    "print(\"\\t\\t\\tadv_BERT_hard \\t adv_biLSTM_hard\")\n",
    "print(\"CNN_hard \\t << \\t\", test_transferability1(\"cnn_hard\", \"transferability/bert_hard.csv\", \"transferability/bilstm_hard.csv\", 245), \"\\n\")\n",
    "\n",
    "print(\"\\t\\t\\tadv_BERT_msda \\t adv_biLSTM_msda\")\n",
    "print(\"CNN_msda \\t << \\t\", test_transferability1(\"cnn_msda\", \"transferability/bert_msda.csv\", \"transferability/bilstm_msda.csv\", 245), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5010e2-0c7a-4465-9f2a-c66c2712063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\t\\t\\t\\t adv_BERT_hard \\t\\t\\t\\t\\t\\t adv_CNN_hard\")\n",
    "print(\"biLSTM_hard \\t << \\t\", test_transferability(\"bilstm_hard\", \"transferability/bert_hard.csv\", \"transferability/cnn_hard.csv\", 245), \"\\n\")\n",
    "\n",
    "print(\"\\t\\t\\t\\t\\t\\t adv_BERT_msda \\t\\t\\t\\t\\t\\t adv_biLSTM_msda\")\n",
    "print(\"biLSTM_msda \\t << \\t\", test_transferability(\"bilstm_msda\", \"transferability/bert_msda.csv\", \"transferability/cnn_msda.csv\", 245), \"\\n\")\n",
    "\n",
    "print(\"\\t\\t\\tadv_BERT_hard \\t adv_CNN_hard\") \n",
    "print(\"biLSTM_hard \\t << \\t\", test_transferability1(\"bilstm_hard\", \"transferability/bert_hard.csv\", \"transferability/cnn_hard.csv\", 245), \"\\n\")\n",
    "\n",
    "print(\"\\t\\t\\tadv_BERT_msda \\t adv_CNN_msda\") \n",
    "print(\"biLSTM_msda \\t << \\t\", test_transferability1(\"bilstm_msda\", \"transferability/bert_msda.csv\", \"transferability/cnn_msda.csv\", 245), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (mldl)",
   "language": "python",
   "name": "mldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
