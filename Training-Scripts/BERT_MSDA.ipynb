{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd2071e-f4e1-40ef-9ba6-f10d409dacfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers -U evaluate arabert #farasapy pyarabic emoji pystemmer optuna==2.3.0\n",
    "! rm -rf BERT_hard\n",
    "! mkdir -p BERT_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6165aa30-749e-4ef0-8946-6642ec7cae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arabert import preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b5afc-55b8-41bd-b854-04d6151415b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('Sentiment_Anaysis.csv')\n",
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217974a6-21f6-43df-8068-e363423697c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(text):\n",
    "    text =re.sub(r'[a-zA-Z?]', '', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "data['Twits'] = data['Twits'].apply(lambda x:clean(x))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emoji\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a22091",
   "metadata": {},
   "outputs": [],
   "source": [
    " with open('emojis.csv','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    emojis_ar = {}\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split(';')\n",
    "        emojis_ar.update({line[0].strip():line[1].strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd8894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals # must be in the begining of the file\n",
    "\n",
    "# Required libraries\n",
    "!pip install unidecode\n",
    "!pip install aiogoogletrans\n",
    "!pip install emoji\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "import emoji\n",
    "from aiogoogletrans import Translator\n",
    "translator = Translator()\n",
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def emoji_native_translation(text):\n",
    "    text = text.lower()\n",
    "    loves = [\"<3\", \"♥\",'❤']\n",
    "    smilefaces = []\n",
    "    sadfaces = []\n",
    "    neutralfaces = []\n",
    "\n",
    "    eyes = [\"8\",\":\",\"=\",\";\"]\n",
    "    nose = [\"'\",\"`\",\"-\",r\"\\\\\"]\n",
    "    for e in eyes:\n",
    "        for n in nose:\n",
    "            for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n",
    "                smilefaces.append(e+n+s)\n",
    "                smilefaces.append(e+s)\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                sadfaces.append(e+n+s)\n",
    "                sadfaces.append(e+s)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(e+n+s)\n",
    "                neutralfaces.append(e+s)\n",
    "            #reversed\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                smilefaces.append(s+n+e)\n",
    "                smilefaces.append(s+e)\n",
    "            for s in [\"\\)\", \"\\]\", \"}\"]:\n",
    "                sadfaces.append(s+n+e)\n",
    "                sadfaces.append(s+e)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(s+n+e)\n",
    "                neutralfaces.append(s+e)\n",
    "\n",
    "    smilefaces = list(set(smilefaces))\n",
    "    sadfaces = list(set(sadfaces))\n",
    "    neutralfaces = list(set(neutralfaces))\n",
    "    t = []\n",
    "    for w in text.split():\n",
    "        if w in loves:\n",
    "            t.append(\"حب\")\n",
    "        elif w in smilefaces:\n",
    "            t.append(\"مضحك\")\n",
    "        elif w in neutralfaces:\n",
    "            t.append(\"عادي\")\n",
    "        elif w in sadfaces:\n",
    "            t.append(\"محزن\")\n",
    "        else:\n",
    "            t.append(w)\n",
    "    newText = \" \".join(t)\n",
    "    return newText\n",
    "\n",
    "def is_emoji(word):\n",
    "    if word in emojis_ar:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "def translate_emojis(words):\n",
    "    word_list = list()\n",
    "    words_to_translate = list()\n",
    "    for word in words :\n",
    "        t = emojis_ar.get(word.get('emoji'),None)\n",
    "        if t is None:\n",
    "            word.update({'translation':'عادي','translated':True})\n",
    "            #words_to_translate.append('normal')\n",
    "        else:\n",
    "            word.update({'translated':False,'translation':t})\n",
    "            words_to_translate.append(t.replace(':','').replace('_',' '))\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def emoji_unicode_translation(text):\n",
    "    text = add_space(text)\n",
    "    words = text.split()\n",
    "    text_list = list()\n",
    "    emojis_list = list()\n",
    "    c = 0\n",
    "    for word in words:\n",
    "        if is_emoji(word):\n",
    "            emojis_list.append({'emoji':word,'emplacement':c})\n",
    "        else:\n",
    "            text_list.append(word)\n",
    "        c+=1\n",
    "    emojis_translated = translate_emojis(emojis_list)\n",
    "    for em in emojis_translated:\n",
    "        text_list.insert(em.get('emplacement'),em.get('translation'))\n",
    "    text = \" \".join(text_list)\n",
    "    return text\n",
    "    \n",
    "def clean_emoji(text):\n",
    "    text = emoji_native_translation(text)\n",
    "    text = emoji_unicode_translation(text)\n",
    "    return text\n",
    "\n",
    "data['Twits'] = data['Twits'].apply(lambda x: clean_emoji(x))\n",
    "#dataset['text'] = dataset['text'].apply(lambda x: clean_emoji(x))\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c392267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition(x):\n",
    "    if x== 'neg':\n",
    "        return -1\n",
    "    elif x=='neu':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    " \n",
    "data['label'] = data['label'].apply(condition)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf9eb98-beb2-4700-8ffc-30d3c47c4faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data[['Twits','label']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f920f-782a-4388-8158-3ea512501306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "model_name=\"bert-base-arabertv2\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "\n",
    "\n",
    "def arabertPreprocessor(text):\n",
    "    text = arabert_prep.preprocess(text)\n",
    "    return text\n",
    "\n",
    "data['process_Twits'] = data['Twits'].apply(lambda x:arabertPreprocessor(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111a379-eca5-480c-ba0b-95689fa8399c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['process_Twits'] = dataset['Twits'].apply(lambda x:arabertPreprocessor(x))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedcc1f5-49c8-4dfd-82ce-7d677b6f69f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['process_Twits'].isnull().values.any(), data['process_Twits'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc0c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset = ['label', 'process_Twits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['process_Twits'].isnull().values.any(), data['process_Twits'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a574aac-caa5-4e86-910b-68b05195a367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['Label'] = pd.Categorical(data.label, ordered=True).codes\n",
    "data['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db31c0-29ee-4d1b-a20c-ed5eae6ebf82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapLabels = pd.DataFrame(data.groupby(['label', 'Label']).count())\n",
    "\n",
    "#drop count column\n",
    "mapLabels.drop(['process_Twits'], axis = 1, inplace = True)\n",
    "label2Index = mapLabels.to_dict(orient='index')\n",
    "\n",
    "print (f\"label2Index :{label2Index}\")\n",
    "print (type(label2Index))\n",
    "#print (f\"index2Label :{index2Label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08041b31-14d1-4292-977d-ff61b73b7cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index2label = {}\n",
    "\n",
    "for key in label2Index:\n",
    "    print (f\"{key[1]} -> {key[0]}\")\n",
    "    index2label[key[1]] = key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01b84d-cc83-488b-a133-a4db6caa7569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label2Index = {v: k for k, v in index2label.items()}\n",
    "\n",
    "print (f'label2Index: {label2Index}')\n",
    "print (f'index2label: {index2label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26d91a-0896-426e-9f8e-9707be3a7931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26064b-a818-4eff-a76e-883a5463b809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification ,BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('aubmindlab/bert-base-arabertv2') #UBC-NLP/MARBERT #aubmindlab/bert-base-arabertv2\n",
    "model = BertForSequenceClassification.from_pretrained('aubmindlab/bert-base-arabertv2', num_labels=3) #UBC-NLP/MARBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4631fc-a0be-4940-8ef6-63362ad0468a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8bb64-27c0-4691-ba5f-01cc08fb0f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = list(data[\"process_Twits\"])\n",
    "y = list(data[\"Label\"])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1,stratify=y)\n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0cea3-505a-4b0a-bee8-7add7860cf72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152af046-24b3-4452-bb42-d7629f7cc533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c130f9d-a5e3-4229-bf76-3ef5c118d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e8d5e-9550-4b9a-aaac-f63c860243e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"BERT_msda\",\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=5000,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb05180e-c694-49ae-a8ad-b7f33cd4ed64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89604798-e070-41aa-af72-f057a0ddf0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "\n",
    "eval_ = pd.DataFrame().append(eval_results, ignore_index=True)\n",
    "eval_ = eval_.rename(columns={\"eval_loss\": \"Evaluation Loss\", \"eval_accuracy\": \"Evaluation Accuracy\"})\n",
    "eval_ = eval_[[\"Evaluation Loss\", \"Evaluation Accuracy\"]] \n",
    "eval_.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8c85e-2fa1-4162-b1e4-4100439e6b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"BERT_msda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
