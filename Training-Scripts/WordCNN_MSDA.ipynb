{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d636cbd7-4ae1-4f78-9579-8418a842c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.python.keras import regularizers\n",
    "from keras.initializers import Constant\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b36b513-87f0-4a19-a8e6-17bb67475ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "! git config --global credential.helper store\n",
    "hf_token='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'\n",
    "login(token=hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca52680-d039-4293-8b87-e325f1bea99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('clean-MSDA.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9e6bb-22a1-4b44-b1aa-734b7a48a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sentences=data.sentences.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d73af-20c0-4d89-8791-ef9e41859b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 330 #300\n",
    "max_features = 20000 # this is the number of words we care about\n",
    "\n",
    "#fit the tokenizer onto the text.\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', filters=' ')\n",
    "tokenizer.fit_on_texts(data['sentences'].values)\n",
    "\n",
    "# this takes our sentences and replaces each word with an integer\n",
    "X = tokenizer.texts_to_sequences(data['sentences'].values)\n",
    "\n",
    "# we then pad the sequences so they're all the same length (sequence_length)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "\n",
    "y = pd.get_dummies(data['label']).values\n",
    "\n",
    "# lets keep  back 10% of the data for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)  \n",
    "\n",
    "print(\"test set size \" + str(len(X_test)))\n",
    "print(\"train set size \" + str(len(X_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695db5d-2d52-4bbe-905e-c210ed96d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings_index = {}\n",
    "f = open('GloVe-Arabic/vectorsMSDA.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:] ,dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf078b7-d5dc-4226-b39c-60e7e1c2605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c7618-7d9f-4688-bbaf-09f1a6562b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "\n",
    "embedding_dim =200\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75155c1-f063-4273-b974-d4d9596bbdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = 100\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "# note the `trainable=False`, later we will make this layer trainable\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=False)(inputs)\n",
    "\n",
    "reshape = Reshape((sequence_length, embedding_dim, 1))(embedding_layer)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "\n",
    "dropout = Dropout(0.3)(flatten)\n",
    "output = Dense(units=3, activation='softmax')(dropout)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72f4df-0d02-48c7-b778-a35f1a62ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42919b-5e79-4f7a-9730-60d5d4047979",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"CNN_MSDA.png\",\n",
    "    show_shapes=True,\n",
    "    show_dtype=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=True,\n",
    "    dpi=90,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef295ee6-9eeb-42bf-8614-273bf37380c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =128   \n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9fd968-0659-4600-86d6-d30f3ac4be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043de6a1-d9ab-43b2-a96c-f38b67aa27bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5c60f-c594-465e-a9c3-fa0f35dcd11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"2dCNNmsda.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508297ff-e7ac-4e13-8ee6-fd5ae25af466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizerCNNmsda.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a318ba-2c5f-40da-9a5b-3b1be3de059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import push_to_hub_keras\n",
    "push_to_hub_keras(model, '2dCNNmsda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c494b1-a86e-4e1c-8034-b1b94f08ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import from_pretrained_keras\n",
    "\n",
    "model_hf = from_pretrained_keras('NorahAlshahrani/2dCNNmsda')\n",
    "model_hf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e483f3b1-4dc1-4bea-9208-1175bf54888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# loaded_model = tf.keras.models.load_model(\"biLSTMhardnew.h5\")\n",
    "tokenizer = pickle.load(open('tokenizerCNNmsda.pickle', 'rb'))\n",
    "\n",
    "text = \"أنا أحب قراءة الكتب\"\n",
    "token = tokenizer.texts_to_sequences([text])\n",
    "token = pad_sequences(token, maxlen=330)\n",
    "\n",
    "outputs=model_hf.predict(token)\n",
    "outputs = torch.from_numpy(outputs)\n",
    "\n",
    "id2label = {\n",
    " 0: 'Negative',\n",
    " 1: 'Netural',\n",
    " 2: 'Positive'\n",
    "\n",
    "}\n",
    "\n",
    "predClassID= outputs.argmax().item()\n",
    "pred = outputs.softmax(dim=-1).tolist()\n",
    "pred = round(np.max(pred)*100, 2)\n",
    "\n",
    "print(f\"Text: '{text}' \\nLabel: {id2label[predClassID]} \\nPredication: {pred}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
