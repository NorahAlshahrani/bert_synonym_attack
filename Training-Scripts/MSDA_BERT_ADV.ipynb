{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb29a59-3e1f-4bb8-ae11-495ff579bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arabert import preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10961a6-c83b-4585-a1d1-b46d4fd5e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "! git config --global credential.helper store\n",
    "hf_token='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'\n",
    "login(token=hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b13b5-0129-430c-9d67-b45ddc1e03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, pandas as pd\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0cc39a-4e5d-4262-ab2d-099d806d11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Sentiment_Anaysis.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069be37f-d97a-454c-a6a6-d5bc032f5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(text):\n",
    "    text =re.sub(r'[a-zA-Z?]', '', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "data['Twits'] = data['Twits'].apply(lambda x:clean(x))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74729914-07a3-4c11-a002-57f192946688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emoji\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15dbed3-43b7-42c3-b093-94dedd2d7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    " with open('emojis.csv','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    emojis_ar = {}\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split(';')\n",
    "        emojis_ar.update({line[0].strip():line[1].strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30579ef-df75-4cd5-87ea-821bff0762a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals # must be in the begining of the file\n",
    "\n",
    "# Required libraries\n",
    "!pip install unidecode\n",
    "!pip install aiogoogletrans\n",
    "!pip install emoji\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "import emoji\n",
    "from aiogoogletrans import Translator\n",
    "translator = Translator()\n",
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def emoji_native_translation(text):\n",
    "    text = text.lower()\n",
    "    loves = [\"<3\", \"♥\",'❤']\n",
    "    smilefaces = []\n",
    "    sadfaces = []\n",
    "    neutralfaces = []\n",
    "\n",
    "    eyes = [\"8\",\":\",\"=\",\";\"]\n",
    "    nose = [\"'\",\"`\",\"-\",r\"\\\\\"]\n",
    "    for e in eyes:\n",
    "        for n in nose:\n",
    "            for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n",
    "                smilefaces.append(e+n+s)\n",
    "                smilefaces.append(e+s)\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                sadfaces.append(e+n+s)\n",
    "                sadfaces.append(e+s)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(e+n+s)\n",
    "                neutralfaces.append(e+s)\n",
    "            #reversed\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                smilefaces.append(s+n+e)\n",
    "                smilefaces.append(s+e)\n",
    "            for s in [\"\\)\", \"\\]\", \"}\"]:\n",
    "                sadfaces.append(s+n+e)\n",
    "                sadfaces.append(s+e)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(s+n+e)\n",
    "                neutralfaces.append(s+e)\n",
    "\n",
    "    smilefaces = list(set(smilefaces))\n",
    "    sadfaces = list(set(sadfaces))\n",
    "    neutralfaces = list(set(neutralfaces))\n",
    "    t = []\n",
    "    for w in text.split():\n",
    "        if w in loves:\n",
    "            t.append(\"حب\")\n",
    "        elif w in smilefaces:\n",
    "            t.append(\"مضحك\")\n",
    "        elif w in neutralfaces:\n",
    "            t.append(\"عادي\")\n",
    "        elif w in sadfaces:\n",
    "            t.append(\"محزن\")\n",
    "        else:\n",
    "            t.append(w)\n",
    "    newText = \" \".join(t)\n",
    "    return newText\n",
    "\n",
    "def is_emoji(word):\n",
    "    if word in emojis_ar:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "def translate_emojis(words):\n",
    "    word_list = list()\n",
    "    words_to_translate = list()\n",
    "    for word in words :\n",
    "        t = emojis_ar.get(word.get('emoji'),None)\n",
    "        if t is None:\n",
    "            word.update({'translation':'عادي','translated':True})\n",
    "            #words_to_translate.append('normal')\n",
    "        else:\n",
    "            word.update({'translated':False,'translation':t})\n",
    "            words_to_translate.append(t.replace(':','').replace('_',' '))\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def emoji_unicode_translation(text):\n",
    "    text = add_space(text)\n",
    "    words = text.split()\n",
    "    text_list = list()\n",
    "    emojis_list = list()\n",
    "    c = 0\n",
    "    for word in words:\n",
    "        if is_emoji(word):\n",
    "            emojis_list.append({'emoji':word,'emplacement':c})\n",
    "        else:\n",
    "            text_list.append(word)\n",
    "        c+=1\n",
    "    emojis_translated = translate_emojis(emojis_list)\n",
    "    for em in emojis_translated:\n",
    "        text_list.insert(em.get('emplacement'),em.get('translation'))\n",
    "    text = \" \".join(text_list)\n",
    "    return text\n",
    "    \n",
    "def clean_emoji(text):\n",
    "    text = emoji_native_translation(text)\n",
    "    text = emoji_unicode_translation(text)\n",
    "    return text\n",
    "\n",
    "data['Twits'] = data['Twits'].apply(lambda x: clean_emoji(x))\n",
    "#dataset['text'] = dataset['text'].apply(lambda x: clean_emoji(x))\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08194b-faf2-4318-9ec2-17103c9a8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition(x):\n",
    "    if x== 'neg':\n",
    "        return -1\n",
    "    elif x=='neu':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    " \n",
    "data['label'] = data['label'].apply(condition)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1d6a2-9e3e-4f36-a141-c85ff8d4f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f559b9-7a8e-453c-b703-d1e1675bd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Twits','label']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183fcef-211e-48ad-8192-3f53752863a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('bert_msda_successful_attacks.csv')\n",
    "dataset=dataset.drop(['model','dataset','predication_score','adversarial_score','targeted_word','synonym_word','example','adversarial_label'],axis=1)\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693daf87-1384-4726-89c4-cc0f7bcda2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns = dataset.columns.str.replace('adversarial_example', 'Twits')\n",
    "dataset.columns = dataset.columns.str.replace('predication_label', 'label')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a866a-2e40-4d01-94ff-0d414d521842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition(x):\n",
    "    if x== 'Negative':\n",
    "        return -1\n",
    "    elif x=='Neutral':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    " \n",
    "dataset['label'] = dataset['label'].apply(condition)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f18fd-5420-4d27-82f9-b8d3511cb008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(text):\n",
    "    text =re.sub(r'[a-zA-Z?]', '', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "dataset['Twits'] = dataset['Twits'].apply(lambda x:clean(x))\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664edae1-5b66-4242-a021-a840a0f684eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "with open('emojis.csv','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    emojis_ar = {}\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split(';')\n",
    "        emojis_ar.update({line[0].strip():line[1].strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e17092-d00f-45c1-aa5a-378e03e50ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals # must be in the begining of the file\n",
    "\n",
    "# Required libraries\n",
    "!pip install unidecode\n",
    "!pip install aiogoogletrans\n",
    "!pip install emoji\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "import emoji\n",
    "from aiogoogletrans import Translator\n",
    "translator = Translator()\n",
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def emoji_native_translation(text):\n",
    "    text = text.lower()\n",
    "    loves = [\"<3\", \"♥\",'❤']\n",
    "    smilefaces = []\n",
    "    sadfaces = []\n",
    "    neutralfaces = []\n",
    "\n",
    "    eyes = [\"8\",\":\",\"=\",\";\"]\n",
    "    nose = [\"'\",\"`\",\"-\",r\"\\\\\"]\n",
    "    for e in eyes:\n",
    "        for n in nose:\n",
    "            for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n",
    "                smilefaces.append(e+n+s)\n",
    "                smilefaces.append(e+s)\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                sadfaces.append(e+n+s)\n",
    "                sadfaces.append(e+s)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(e+n+s)\n",
    "                neutralfaces.append(e+s)\n",
    "            #reversed\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                smilefaces.append(s+n+e)\n",
    "                smilefaces.append(s+e)\n",
    "            for s in [\"\\)\", \"\\]\", \"}\"]:\n",
    "                sadfaces.append(s+n+e)\n",
    "                sadfaces.append(s+e)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(s+n+e)\n",
    "                neutralfaces.append(s+e)\n",
    "\n",
    "    smilefaces = list(set(smilefaces))\n",
    "    sadfaces = list(set(sadfaces))\n",
    "    neutralfaces = list(set(neutralfaces))\n",
    "    t = []\n",
    "    for w in text.split():\n",
    "        if w in loves:\n",
    "            t.append(\"حب\")\n",
    "        elif w in smilefaces:\n",
    "            t.append(\"مضحك\")\n",
    "        elif w in neutralfaces:\n",
    "            t.append(\"عادي\")\n",
    "        elif w in sadfaces:\n",
    "            t.append(\"محزن\")\n",
    "        else:\n",
    "            t.append(w)\n",
    "    newText = \" \".join(t)\n",
    "    return newText\n",
    "\n",
    "def is_emoji(word):\n",
    "    if word in emojis_ar:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "def translate_emojis(words):\n",
    "    word_list = list()\n",
    "    words_to_translate = list()\n",
    "    for word in words :\n",
    "        t = emojis_ar.get(word.get('emoji'),None)\n",
    "        if t is None:\n",
    "            word.update({'translation':'عادي','translated':True})\n",
    "            #words_to_translate.append('normal')\n",
    "        else:\n",
    "            word.update({'translated':False,'translation':t})\n",
    "            words_to_translate.append(t.replace(':','').replace('_',' '))\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def emoji_unicode_translation(text):\n",
    "    text = add_space(text)\n",
    "    words = text.split()\n",
    "    text_list = list()\n",
    "    emojis_list = list()\n",
    "    c = 0\n",
    "    for word in words:\n",
    "        if is_emoji(word):\n",
    "            emojis_list.append({'emoji':word,'emplacement':c})\n",
    "        else:\n",
    "            text_list.append(word)\n",
    "        c+=1\n",
    "    emojis_translated = translate_emojis(emojis_list)\n",
    "    for em in emojis_translated:\n",
    "        text_list.insert(em.get('emplacement'),em.get('translation'))\n",
    "    text = \" \".join(text_list)\n",
    "    return text\n",
    "    \n",
    "def clean_emoji(text):\n",
    "    text = emoji_native_translation(text)\n",
    "    text = emoji_unicode_translation(text)\n",
    "    return text\n",
    "\n",
    "dataset['Twits'] = dataset['Twits'].apply(lambda x: clean_emoji(x))\n",
    "#dataset['text'] = dataset['text'].apply(lambda x: clean_emoji(x))\n",
    "\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca24a1-cdb9-4e76-8901-d0166d41f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b97682-bb07-4213-a977-433c206b475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[['Twits','label']]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842dfaae-5ad5-469a-8c25-b48d5cbce45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "model_name=\"bert-base-arabertv2\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "\n",
    "\n",
    "def arabertPreprocessor(text):\n",
    "    text = arabert_prep.preprocess(text)\n",
    "    return text\n",
    "\n",
    "data['process_Twits'] = data['Twits'].apply(lambda x:arabertPreprocessor(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78515721-c305-4bc6-a4da-386b6bc758af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['process_Twits'] = dataset['Twits'].apply(lambda x:arabertPreprocessor(x))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d7630-3c36-4e47-bf6c-44753388a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['process_Twits'].isnull().values.any(), data['process_Twits'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783542e7-def2-405a-809a-d32ac0c57383",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['process_Twits'].isnull().values.any(), dataset['process_Twits'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc6113-4b99-4da7-a554-25058f58c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset = ['label', 'process_Twits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75e09f-647a-496a-9a81-0b6c1387d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(subset = ['label', 'process_Twits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133d96c-a3ba-4dfb-9989-c1371d4627b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['process_Twits'].isnull().values.any(), data['process_Twits'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040553ad-1b39-43d8-8daf-c28c5a761922",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['process_Twits'].isnull().values.any(), dataset['process_Twits'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21137d20-4c3b-4840-8bf8-7494792c5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Label'] = pd.Categorical(data.label, ordered=True).codes\n",
    "data['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0bd6ca-1512-416e-8153-3fd4cd11a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Label'] = pd.Categorical(dataset.label, ordered=True).codes\n",
    "dataset['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791dcd5e-434b-434e-9bb5-6ecbe42cf347",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapLabels = pd.DataFrame(data.groupby(['label', 'Label']).count())\n",
    "\n",
    "#drop count column\n",
    "mapLabels.drop(['process_Twits'], axis = 1, inplace = True)\n",
    "label2Index = mapLabels.to_dict(orient='index')\n",
    "\n",
    "print (f\"label2Index :{label2Index}\")\n",
    "print (type(label2Index))\n",
    "#print (f\"index2Label :{index2Label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89835e56-16fa-4d7c-91b8-39a3b98e1aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2label = {}\n",
    "\n",
    "for key in label2Index:\n",
    "    print (f\"{key[1]} -> {key[0]}\")\n",
    "    index2label[key[1]] = key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3b247f-2393-4706-92f5-3bba48392d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2Index = {v: k for k, v in index2label.items()}\n",
    "\n",
    "print (f'label2Index: {label2Index}')\n",
    "print (f'index2label: {index2label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cfe48b-2801-4e79-b383-05d6aba2a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapLabels = pd.DataFrame(dataset.groupby(['label', 'Label']).count())\n",
    "\n",
    "#drop count column\n",
    "mapLabels.drop(['process_Twits'], axis = 1, inplace = True)\n",
    "label3Index = mapLabels.to_dict(orient='index')\n",
    "\n",
    "print (f\"label3Index :{label3Index}\")\n",
    "print (type(label3Index))\n",
    "#print (f\"index2Label :{index2Label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5d9e5-ebe3-451a-8452-7c2202bdb2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "index3label = {}\n",
    "\n",
    "for key in label3Index:\n",
    "    print (f\"{key[1]} -> {key[0]}\")\n",
    "    index3label[key[1]] = key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928fa138-a855-43e3-b51e-6a4d5df7e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "label3Index = {v: k for k, v in index3label.items()}\n",
    "\n",
    "print (f'label3Index: {label3Index}')\n",
    "print (f'index3label: {index3label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747aaf3-bbc4-46cf-8177-7c811bc17d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67094333-a445-4b61-835b-74912416e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification ,BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('aubmindlab/bert-base-arabertv2') #UBC-NLP/MARBERT #aubmindlab/bert-base-arabertv2\n",
    "model = BertForSequenceClassification.from_pretrained('aubmindlab/bert-base-arabertv2', num_labels=3) #UBC-NLP/MARBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb46a2-8c67-4b61-ab4a-e1a036f6c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e5cff-fc83-4e8d-bdb6-e9abf1c6f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(data[\"process_Twits\"])\n",
    "y = list(data[\"Label\"])\n",
    "\n",
    "O_X_train, O_X_val, O_y_train, O_y_val = train_test_split(X, y, test_size=0.1,stratify=y)\n",
    "len(O_X_train),len(O_X_val), len(O_y_train),len(O_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954780ed-9f9d-43e2-9ce5-0d24158cddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ADV = list(dataset[\"process_Twits\"])\n",
    "y_ADV = list(dataset[\"Label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd9f02-c758-4cdd-aba6-83ec4598a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = O_X_train + X_ADV\n",
    "X_val = O_X_val \n",
    "y_train = O_y_train + y_ADV\n",
    "y_val = O_y_val \n",
    "\n",
    "len(X_train), len(X_val),len(y_train), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d08efa-3259-467a-b533-7d883f1d40ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c9158-78a6-493a-8e71-ee41b1b7f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx]) #labels\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08447a-2009-48ae-9793-1a9213c4e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8280c85-63e0-49df-b644-e5d63770e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2864fff8-e95f-4a9c-af45-1352359511d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate>=0.20.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef90cc8-f5f1-490d-8452-f5a5c173ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"Adv_BERT_msda\",\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=5000,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638aa58c-89a6-425e-9d5a-9b1c97c06969",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c690a373-4712-49eb-ac0c-365bbec9c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "\n",
    "eval_ = pd.DataFrame().append(eval_results, ignore_index=True)\n",
    "eval_ = eval_.rename(columns={\"eval_loss\": \"Evaluation Loss\", \"eval_accuracy\": \"Evaluation Accuracy\"})\n",
    "eval_ = eval_[[\"Evaluation Loss\", \"Evaluation Accuracy\"]] \n",
    "eval_.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e248bcc-2a6c-41c4-bd02-39e4f52489e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"Adv_BERT_msda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ed699-e6fd-4fb4-9148-a5f328613aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub('NorahAlshahrani/Adv_BERT_msda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e087a62-c39a-4b91-9e19-5e01bad8dcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
