{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35897305-8e61-4309-888a-4f9475d39713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pickle\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.initializers import Constant\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974497b-bb9e-4962-967c-8bd2fa38009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_visible_devices(devices=gpus[0], device_type='GPU')\n",
    "tf.config.experimental.set_memory_growth(device=gpus[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f2393-ac87-496b-9674-c99a22963b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "! git config --global credential.helper store\n",
    "hf_token='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'\n",
    "login(token=hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c4b2d-5f6c-4ccb-a783-5fcc0427d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('clean-MSDA.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a81202-9da5-47a3-8290-debf6637edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sentences=data.sentences.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b80120-4d00-444d-a1ae-0d3e8f101e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 330\n",
    "max_features = 20000 # this is the number of words we care about\n",
    "\n",
    "#fit the tokenizer onto the text.\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', filters=' ')\n",
    "tokenizer.fit_on_texts(data['sentences'].values)\n",
    "\n",
    "# this takes our sentences and replaces each word with an integer\n",
    "X = tokenizer.texts_to_sequences(data['sentences'].values)\n",
    "\n",
    "# we then pad the sequences so they're all the same length (sequence_length)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "\n",
    "y = pd.get_dummies(data['label']).values\n",
    "\n",
    "# lets keep  back 20% of the data for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)  \n",
    "\n",
    "print(\"train set size \" + str(len(X_train)))\n",
    "print(\"test set size \" + str(len(X_test)))\n",
    "#print(X_train[0] , y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc8f7b-373f-4064-8ad9-9ea7c23fdadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings_index = {}\n",
    "f = open('GloVe-Arabic/vectorsMSDA.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:] ,dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3f71b-8ed2-4afa-b861-9af68a85b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962a9d1c-356a-432b-ad4a-797bff8d5084",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "\n",
    "embedding_dim =200\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4addc20-6af1-4091-af2f-162c02827ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(num_words,\n",
    "                             embedding_dim,\n",
    "                             embeddings_initializer=Constant(embedding_matrix),\n",
    "                             input_length=sequence_length,\n",
    "                             trainable=False),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150)),\n",
    "    tf.keras.layers.Dropout(0.8),\n",
    "    tf.keras.layers.Dense(units=3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d94e7-7548-4ab0-b101-df80f6d763d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"biLSTMmsda.png\",\n",
    "    show_shapes=True,\n",
    "    show_dtype=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=True,\n",
    "    dpi=90,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2edb72d-4243-4f92-964f-24402543238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b656f9-1751-46d0-947c-aa30276cf9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2deb735-eb26-47f0-b034-617a291c5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d0acf3-44d3-4a00-b6a2-a9ffa75357a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"biLSTMmsda.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115e79b-cf7d-4e84-9ee3-eec24e111a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizerbiLSTMmsda.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afde4f-d08c-4864-a429-dcda9d24c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import push_to_hub_keras\n",
    "push_to_hub_keras(model, 'biLSTMmsda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ae24b-b958-4a12-a1f5-0a8c3314a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import from_pretrained_keras\n",
    "\n",
    "model_hf = from_pretrained_keras('NorahAlshahrani/biLSTMmsda')\n",
    "model_hf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453416c-231f-498f-b418-1a5d0c7b2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# loaded_model = tf.keras.models.load_model(\"biLSTMhardnew.h5\")\n",
    "tokenizer = pickle.load(open('tokenizerbiLSTMmsda.pickle', 'rb'))\n",
    "\n",
    "text = \"أنا أحب قراءة الكتب\"\n",
    "token = tokenizer.texts_to_sequences([text])\n",
    "token = pad_sequences(token, maxlen=330)\n",
    "\n",
    "outputs=model_hf.predict(token)\n",
    "outputs = torch.from_numpy(outputs)\n",
    "\n",
    "id2label = {\n",
    " 0: 'Negative',\n",
    " 1: 'Netural',\n",
    " 2: 'Positive'\n",
    "\n",
    "}\n",
    "\n",
    "predClassID= outputs.argmax().item()\n",
    "pred = outputs.softmax(dim=-1).tolist()\n",
    "pred = round(np.max(pred)*100, 2)\n",
    "\n",
    "print(f\"Text: '{text}' \\nLabel: {id2label[predClassID]} \\nPredication: {pred}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212fc5da-5297-4a09-b6bc-4fa0234b0c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
