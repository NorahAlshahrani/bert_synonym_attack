{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62611a-ca96-459d-aee8-49e991d2b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.python.keras import regularizers\n",
    "from keras.initializers import Constant\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42e573-14aa-4e0a-8468-82e928cf5345",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('clean-HARD.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53856cb-1364-43d4-a3a4-04e1ffa42be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sentences=data.sentences.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fb16e-680d-4beb-9f7a-dc61519f504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 512 #300\n",
    "max_features = 20000 # this is the number of words we care about\n",
    "\n",
    "#fit the tokenizer onto the text.\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', filters=' ')\n",
    "tokenizer.fit_on_texts(data['sentences'].values)\n",
    "\n",
    "# this takes our sentences and replaces each word with an integer\n",
    "X = tokenizer.texts_to_sequences(data['sentences'].values)\n",
    "\n",
    "# we then pad the sequences so they're all the same length (sequence_length)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "\n",
    "y = pd.get_dummies(data['rating']).values\n",
    "\n",
    "# lets keep  back 20% of the data for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)  \n",
    "\n",
    "print(\"test set size \" + str(len(X_test)))\n",
    "print(\"train set size \" + str(len(X_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ab442-0ff4-426f-8a3e-37e9a2935e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings_index = {}\n",
    "f = open('GloVe-Arabic/vectorsHARD.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:] ,dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e189f3-e8b7-466f-8bad-3a9104ac60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a76281-8e42-40d5-954a-7ff86e4ce0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "\n",
    "embedding_dim =200\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd169242-969f-49ed-9b3c-97f545b17d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = 100\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "# note the `trainable=False`, later we will make this layer trainable\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=False)(inputs)\n",
    "\n",
    "reshape = Reshape((sequence_length, embedding_dim, 1))(embedding_layer)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "\n",
    "dropout = Dropout(0.1)(flatten)\n",
    "output = Dense(units=4, activation='softmax')(dropout)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701e50c-a86a-49d1-b135-7e3d4ffc4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57c45d-fd4d-4468-b38c-639ab9e6ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"CNN_HARD.png\",\n",
    "    show_shapes=True,\n",
    "    show_dtype=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=True,\n",
    "    dpi=90,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a75b3d-71f0-4f71-b510-dc10405a861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =128   \n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bea325d-be58-4695-96d4-53e660ed1525",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008df363-cfa6-47e2-8828-b357b87bd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed6358-da42-417a-bfd2-91647ece5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"2dCNNhard.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f57cc3-ed2c-4fae-9e43-a0308082ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizerCNNhard.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc118b-ac59-499f-80f8-d3a946836a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import push_to_hub_keras\n",
    "push_to_hub_keras(model, '2dCNNhard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f10c53-7515-4db1-9457-80d9a289b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import from_pretrained_keras\n",
    "\n",
    "model_hf = from_pretrained_keras('NorahAlshahrani/2dCNNhard')\n",
    "model_hf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71bae61-ca0c-4d0a-b476-22fc3e9a2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# loaded_model = tf.keras.models.load_model(\"biLSTMhardnew.h5\")\n",
    "tokenizer = pickle.load(open('tokenizerCNNhard.pickle', 'rb'))\n",
    "\n",
    "text = \"أنا لا أحب قراءة الكتب\"\n",
    "token = tokenizer.texts_to_sequences([text])\n",
    "token = pad_sequences(token, maxlen=512)\n",
    "\n",
    "outputs=model_hf.predict(token)\n",
    "outputs = torch.from_numpy(outputs)\n",
    "\n",
    "id2label = {\n",
    " 0: 'Negative',\n",
    " 1: 'Negative',\n",
    " 2: 'Positive',\n",
    " 3: 'Positive'\n",
    "}\n",
    "\n",
    "predClassID= outputs.argmax().item()\n",
    "pred = outputs.softmax(dim=-1).tolist()\n",
    "pred = round(np.max(pred)*100, 2)\n",
    "\n",
    "print(f\"Text: '{text}' \\nLabel: {id2label[predClassID]} \\nPredication: {pred}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
